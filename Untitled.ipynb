{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce2d187-2f53-4c54-8171-e00519bf6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd50dd7-f880-4cbc-890e-511851fcfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the model\n",
    "# def create_model(input_shape, num_classes):\n",
    "#     model = models.Sequential([\n",
    "#         layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "#         layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "#         layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(64, activation='relu'),\n",
    "#         layers.Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396fe3b9-fbdd-4484-a43c-7295998f0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the create_model function to accept input_shape\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d99b08-7529-4c5d-8aa1-09b257cfcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to load and preprocess audio files\n",
    "# def preprocess_data(data_dir, num_classes, test_size=0.2):\n",
    "#     X = []\n",
    "#     y = []\n",
    "    \n",
    "#     # Iterate through each subdirectory (each voice type)\n",
    "#     for label in os.listdir(data_dir):\n",
    "#         label_dir = os.path.join(data_dir, label)\n",
    "        \n",
    "#         # Iterate through each audio file in the subdirectory\n",
    "#         for file in os.listdir(label_dir):\n",
    "#             file_path = os.path.join(label_dir, file)\n",
    "            \n",
    "#             # Load the audio file and convert it to a spectrogram\n",
    "#             y_, sr = librosa.load(file_path)\n",
    "#             spectrogram = librosa.feature.melspectrogram(y=y_, sr=sr)\n",
    "#             spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "            \n",
    "#             # Resize the spectrogram to a fixed size (if necessary)\n",
    "#             # spectrogram = resize(spectrogram, (desired_height, desired_width))\n",
    "            \n",
    "#             X.append(spectrogram)\n",
    "#             y.append(int(label))  # Assuming label directories are named with integers\n",
    "    \n",
    "#     # Convert lists to numpy arrays\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "    \n",
    "#     # Split the data into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0a9603-1746-47a4-947c-d4dea8179c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess audio files\n",
    "def preprocess_data(data_dir, test_size=0.2, desired_shape=(128, 128)):\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = {}\n",
    "    \n",
    "    # Iterate through each subdirectory (each voice type)\n",
    "    for i, label in enumerate(os.listdir(data_dir)):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        labels[i] = label  # Store the label for reference\n",
    "        \n",
    "        # Iterate through each audio file in the subdirectory\n",
    "        for file in os.listdir(label_dir):\n",
    "            file_path = os.path.join(label_dir, file)\n",
    "            \n",
    "            # Load the audio file and convert it to a spectrogram\n",
    "            y_, sr = librosa.load(file_path)\n",
    "            spectrogram = librosa.feature.melspectrogram(y=y_, sr=sr)\n",
    "            spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "            \n",
    "            # Resize the spectrogram to a fixed size\n",
    "            pad_width = desired_shape[1] - spectrogram.shape[1]\n",
    "            if pad_width > 0:\n",
    "                spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                spectrogram = spectrogram[:, :desired_shape[1]]\n",
    "            \n",
    "            X.append(spectrogram)\n",
    "            y.append(i)  # Use label index as the target value\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Add batch dimension to the input data\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993b8f42-106c-419e-8010-9f472664cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 28s 1s/step - loss: 9.5294 - accuracy: 0.4524 - val_loss: 0.4997 - val_accuracy: 0.8214\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 21s 993ms/step - loss: 0.4423 - accuracy: 0.8274 - val_loss: 0.3426 - val_accuracy: 0.8810\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 14s 683ms/step - loss: 0.2452 - accuracy: 0.9330 - val_loss: 0.2555 - val_accuracy: 0.9464\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 12s 586ms/step - loss: 0.1500 - accuracy: 0.9583 - val_loss: 0.2487 - val_accuracy: 0.9405\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 10s 469ms/step - loss: 0.0956 - accuracy: 0.9658 - val_loss: 0.2095 - val_accuracy: 0.9524\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 11s 518ms/step - loss: 0.1096 - accuracy: 0.9568 - val_loss: 0.2172 - val_accuracy: 0.9345\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 9s 448ms/step - loss: 0.0951 - accuracy: 0.9732 - val_loss: 0.2763 - val_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 10s 481ms/step - loss: 0.1920 - accuracy: 0.9345 - val_loss: 0.3727 - val_accuracy: 0.8929\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 10s 464ms/step - loss: 0.1182 - accuracy: 0.9688 - val_loss: 0.2187 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 10s 494ms/step - loss: 0.0760 - accuracy: 0.9762 - val_loss: 0.2385 - val_accuracy: 0.9405\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.2385 - accuracy: 0.9405\n",
      "Test accuracy: 0.9404761791229248\n",
      "Labels: {0: 'alto', 1: 'bass', 2: 'soprano', 3: 'tenor'}\n"
     ]
    }
   ],
   "source": [
    "# Set the directory containing your dataset\n",
    "data_dir = 'dataset'\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test, labels = preprocess_data(data_dir)\n",
    "\n",
    "# Ensure the input shape is correct\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(input_shape, len(labels))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Print the labels\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e947f81-9522-41a3-9273-a2d315a6ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_file, desired_shape=(128, 128)):\n",
    "    # Load the audio file and convert it to a spectrogram\n",
    "    y_, sr = librosa.load(audio_file)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y_, sr=sr)\n",
    "    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    \n",
    "    # Resize the spectrogram to the desired shape\n",
    "    current_shape = spectrogram.shape\n",
    "    if current_shape[1] > desired_shape[1]:\n",
    "        # If the spectrogram has more columns than desired, trim it\n",
    "        spectrogram = spectrogram[:, :desired_shape[1]]\n",
    "    elif current_shape[1] < desired_shape[1]:\n",
    "        # If the spectrogram has fewer columns than desired, pad it\n",
    "        pad_width = desired_shape[1] - current_shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    \n",
    "    # Resize the spectrogram to the desired number of rows\n",
    "    if current_shape[0] != desired_shape[0]:\n",
    "        spectrogram = librosa.util.fix_length(spectrogram, desired_shape[0], axis=0)\n",
    "    \n",
    "    # Add channel dimension\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "    \n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45848992-463b-4089-b8f4-ac0a3ca4a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply time stretching\n",
    "def time_stretch(audio, rate):\n",
    "    stretched_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "    return stretched_audio\n",
    "\n",
    "# Function to apply pitch shifting\n",
    "def pitch_shift(audio, sr, n_steps):\n",
    "    shifted_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "    return shifted_audio\n",
    "\n",
    "# Function to add background noise\n",
    "def add_background_noise(audio, noise_factor):\n",
    "    # Load background noise audio file (e.g., white noise)\n",
    "    noise_audio, _ = librosa.load('background/crowd_bg.wav', sr=None)\n",
    "    # Ensure noise audio length is at least as long as the original audio\n",
    "    if len(noise_audio) < len(audio):\n",
    "        repeat_times = int(np.ceil(len(audio) / len(noise_audio)))\n",
    "        noise_audio = np.tile(noise_audio, repeat_times)[:len(audio)]\n",
    "    # Add noise to the original audio\n",
    "    noisy_audio = audio + noise_factor * noise_audio[:len(audio)]\n",
    "    return noisy_audio\n",
    "\n",
    "# Function to apply time shifting\n",
    "def time_shift(audio, sr, max_shift_ms=100):\n",
    "    # Convert maximum shift from milliseconds to samples\n",
    "    max_shift_samples = int(max_shift_ms * sr / 1000)\n",
    "    # Generate random shift amount\n",
    "    shift_amount = random.randint(-max_shift_samples, max_shift_samples)\n",
    "    # Apply time shift\n",
    "    shifted_audio = np.roll(audio, shift_amount)\n",
    "    return shifted_audio\n",
    "\n",
    "# Function to change speed\n",
    "def change_speed(audio, speed_factor):\n",
    "    # Resample the audio with the speed factor\n",
    "    sped_audio = librosa.effects.time_stretch(audio, rate=speed_factor)\n",
    "    return sped_audio\n",
    "\n",
    "# Function to apply audio filters (e.g., reverb, echo, equalization)\n",
    "def apply_audio_filter(audio):\n",
    "    # Apply a random filter to the audio (e.g., reverb, echo, equalization)\n",
    "    # Example: apply reverb filter\n",
    "    reverb_audio = audio * np.random.uniform(0.5, 1.5)\n",
    "    return reverb_audio\n",
    "\n",
    "# Function to randomly crop or pad the audio\n",
    "def random_crop_or_pad(audio, target_length):\n",
    "    # Randomly crop or pad the audio to the target length\n",
    "    if len(audio) < target_length:\n",
    "        # Pad the audio\n",
    "        pad_length = target_length - len(audio)\n",
    "        padded_audio = np.pad(audio, (0, pad_length), mode='constant')\n",
    "        return padded_audio\n",
    "    elif len(audio) > target_length:\n",
    "        # Randomly crop the audio\n",
    "        start_idx = np.random.randint(0, len(audio) - target_length)\n",
    "        cropped_audio = audio[start_idx:start_idx + target_length]\n",
    "        return cropped_audio\n",
    "    else:\n",
    "        return audio\n",
    "\n",
    "# Function to resample the audio\n",
    "def resample_audio(audio, target_sr):\n",
    "    resampled_audio = librosa.resample(audio, orig_sr=len(audio), target_sr=target_sr)\n",
    "    return resampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf212806-c519-4ea9-9360-62d428c710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load an example audio file\n",
    "# audio_file = 'bassvoice.wav'\n",
    "# audio, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# # Apply data augmentation techniques\n",
    "# augmented_audios = []\n",
    "# for i in range(10):  # Generate 10 augmented samples\n",
    "#     augmented_audio = audio.copy()  # Make a copy of the original audio\n",
    "\n",
    "#     # Apply random data augmentation techniques\n",
    "#     rate = np.random.uniform(0.8, 1.2)\n",
    "#     augmented_audio = time_stretch(augmented_audio, rate)\n",
    "#     augmented_audio = pitch_shift(augmented_audio, sr, n_steps=np.random.randint(-3, 3))\n",
    "#     augmented_audio = add_background_noise(augmented_audio, noise_factor=np.random.uniform(0.001, 0.01))\n",
    "#     augmented_audio = time_shift(augmented_audio, sr, max_shift_ms=50)\n",
    "#     augmented_audio = change_speed(augmented_audio, speed_factor=np.random.uniform(0.8, 1.2))\n",
    "#     augmented_audio = apply_audio_filter(augmented_audio)\n",
    "#     augmented_audio = random_crop_or_pad(augmented_audio, target_length=len(audio))\n",
    "#     augmented_audio = resample_audio(augmented_audio, target_sr=sr)\n",
    "\n",
    "#     augmented_audios.append(augmented_audio)\n",
    "\n",
    "# # Save or use augmented audio samples as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fa97b0-3a1d-4cbe-81fc-692754a40b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the voice recording\u001b[39;00m\n\u001b[0;32m      2\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtenor_voice.wav\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Change this to your voice recording file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m preprocessed_audio \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_audio\u001b[49m(audio_file)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ensure that the input shape matches the model's input shape\u001b[39;00m\n\u001b[0;32m      6\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m preprocessed_audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_audio' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess the voice recording\n",
    "audio_file = 'tenor_voice.wav'  # Change this to your voice recording file\n",
    "preprocessed_audio = preprocess_audio(audio_file)\n",
    "\n",
    "# Ensure that the input shape matches the model's input shape\n",
    "input_shape = preprocessed_audio.shape[1:]\n",
    "\n",
    "# Make predictions using your existing model\n",
    "predictions = model.predict(np.expand_dims(preprocessed_audio, axis=0))\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "predicted_class = labels[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0afe2fed-9d64-43e7-ba4b-dca9e250738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory containing WAV files to augment\n",
    "# data_dir = 'dataset/bass'\n",
    "\n",
    "# # Iterate through each WAV file in the directory\n",
    "# for file_name in os.listdir(data_dir):\n",
    "#     if file_name.endswith('.wav'):\n",
    "#         file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "#         # Load the original WAV file\n",
    "#         audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "#         # Apply data augmentation techniques\n",
    "#         rate = np.random.uniform(0.8, 1.2)\n",
    "#         augmented_audio = time_stretch(audio, rate)\n",
    "#         augmented_audio = pitch_shift(augmented_audio, sr, n_steps=np.random.randint(-3, 3))\n",
    "#         augmented_audio = add_background_noise(augmented_audio, noise_factor=np.random.uniform(0.001, 0.01))\n",
    "#         augmented_audio = time_shift(augmented_audio, sr, max_shift_ms=50)\n",
    "#         augmented_audio = change_speed(augmented_audio, speed_factor=np.random.uniform(0.8, 1.2))\n",
    "#         augmented_audio = apply_audio_filter(augmented_audio)\n",
    "#         augmented_audio = random_crop_or_pad(augmented_audio, target_length=len(audio))\n",
    "#         augmented_audio = resample_audio(augmented_audio, target_sr=sr)\n",
    "        \n",
    "#         # Save the augmented audio\n",
    "#         output_file = os.path.join(data_dir, 'augmented_' + file_name)\n",
    "#         sf.write(output_file, augmented_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34523aa7-7a86-4edf-aba9-021215bdf984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory containing WAV files to augment\n",
    "\n",
    "# classlist = ['bass', 'tenor', 'alto', 'soprano']\n",
    "\n",
    "\n",
    "\n",
    "# # Iterate through each WAV file in the directory\n",
    "# for class_voice in classlist:\n",
    "#     data_dir = 'dataset/'+class_voice\n",
    "#     for file_name in os.listdir(data_dir):\n",
    "#         if file_name.endswith('.wav'):\n",
    "#             file_path = os.path.join(data_dir, file_name)\n",
    "            \n",
    "#             # Load the original WAV file\n",
    "#             audio, sr = librosa.load(file_path, sr=None)\n",
    "            \n",
    "#             # Apply data augmentation techniques\n",
    "#             rate = np.random.uniform(0.8, 1.2)\n",
    "            \n",
    "#             # Load the original WAV file\n",
    "#             audio, sr = librosa.load(file_path, sr=None)\n",
    "            \n",
    "#             # Apply time stretching\n",
    "#             rate = np.random.uniform(0.8, 1.2)\n",
    "#             augmented_audio = time_stretch(audio, rate)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'time_stretch_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Apply pitch shifting\n",
    "#             n_steps = np.random.randint(-3, 3)\n",
    "#             augmented_audio = pitch_shift(audio, sr, n_steps=n_steps)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'pitch_shift_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Apply background noise\n",
    "#             noise_factor = np.random.uniform(0.001, 0.01)\n",
    "#             augmented_audio = add_background_noise(audio, noise_factor=noise_factor)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'background_noise_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Apply time shifting\n",
    "#             augmented_audio = time_shift(audio, sr, max_shift_ms=50)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'time_shift_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Apply speed change\n",
    "#             speed_factor = np.random.uniform(0.8, 1.2)\n",
    "#             augmented_audio = change_speed(audio, speed_factor=speed_factor)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'change_speed_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Apply audio filter\n",
    "#             augmented_audio = apply_audio_filter(audio)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'audio_filter_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Randomly crop or pad the audio\n",
    "#             augmented_audio = random_crop_or_pad(audio, target_length=len(audio))\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'random_crop_pad_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "    \n",
    "#             # Resample the audio\n",
    "#             augmented_audio = resample_audio(audio, target_sr=sr)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'resampled_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)\n",
    "            \n",
    "#             # Save the augmented audio\n",
    "#             output_file = os.path.join(data_dir, 'augmented_' + file_name)\n",
    "#             sf.write(output_file, augmented_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ecaca-ec30-4dc2-8143-af73860fd756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
